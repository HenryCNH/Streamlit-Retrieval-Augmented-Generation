import tempfile
from typing import List, TypedDict

import streamlit as st
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langgraph.graph import END, StateGraph

# ----------------------------------------------------Now we define the vectorstore and retrieval process -------------------------------------------------------------------------------


class PDF_Importer:
    def __init__(self, pdfs):
        self.pdfs = pdfs
        self.docs_list = self.get_docs()

        self.text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            chunk_size=50, chunk_overlap=20
        )
        self.doc_splits = self.text_splitter.split_documents(self.docs_list)

        self.embeddings = OllamaEmbeddings(model="nomic-embed-text")

        self.vectorstore = FAISS.from_documents(
            documents=self.doc_splits,
            docstore=InMemoryDocstore(),
            embedding=self.embeddings,
        )

    def get_docs(self):
        docs_list = []
        for pdf_file in self.pdfs:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_pdf:
                temp_pdf.write(pdf_file.getvalue())
                temp_pdf.flush()
                docs_list.append(PyPDFLoader(temp_pdf.name).load())
        docs_list = [item for sublist in docs_list for item in sublist]
        return docs_list

    def get_retriever(self):
        return self.vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 5})


# --------------------------------After defining the vectorstore and its retrieval process, we can now define the graph for the chatbot-------------------------------------------------


class GraphState(TypedDict):
    user_query: str
    response: str
    retrieved_chunks: List[str]


class PdfChat:
    def __init__(self, retriever, selected_model):

        self.model = OllamaLLM(model=selected_model)

        builder = StateGraph(GraphState)
        builder.add_node("retrieve", self.retriever_node)
        builder.add_node("fine_tune_question", self.fine_tune_user_query)
        builder.add_node("standardize_document", self.standardize_retrieved_document)
        builder.add_node("generate_final_output", self.generate_final_output)

        builder.set_entry_point("fine_tune_question")

        builder.add_edge("fine_tune_question", "retrieve")
        builder.add_edge("retrieve", "standardize_document")
        builder.add_edge("standardize_document", "generate_final_output")
        builder.add_edge("generate_final_output", END)

        self.retriever = retriever

        self.graph = builder.compile()

        self.graph.get_graph().draw_mermaid_png(output_file_path="graph.png")
        self.memory = ConversationBufferMemory()

    def fine_tune_user_query(self, state: GraphState):
        with st.spinner(
            "Fine tuning your query for vectorstore similarity matching..."
        ):
            question = state["user_query"]
            memory = self.memory.load_memory_variables({})
            prompt = """###Task:
                        You are responsible for standardizing the user query that will be used to search 
                        for similarity search to search for chunks with similar
                        words in the vectorstore of a retrieval augmented generation application.
                        You will be given the previous conversation history and the user query.
                        For all the specific term, you have to stick to the original term.
                        Do not make up new terms.
                        ### Previous Conversation history:
                        {memory} 
                        ### User Query:{question}
                    """
            prompt = PromptTemplate.from_template(prompt)
            chain = prompt | self.model | StrOutputParser()

            question = chain.invoke({"question": question, "memory": memory})

            return {"user_query": question}

    def retriever_node(self, state: GraphState):
        question = state["user_query"]

        documents = self.retriever.invoke(question)
        if not documents:
            return "I couldn't find any relevant documents. Can you please rephrase your question?"

        return {"retrieved_chunks": documents}

    def standardize_retrieved_document(self, state: GraphState):
        with st.spinner("Standardizing the retrieved documents..."):
            documents = state["retrieved_chunks"]
            question = state["user_query"]
            documents = [doc.page_content for doc in documents]

            prompt = """
                        ### Task: Document Restructuring for RAG - Optimized for LLM Generation 

                        You are an expert document restructurer for a Retrieval Augmented Generation (RAG) application. Your primary goal is to process 
                        retrieved chunks from a vector store and refine them to best answer the user's query. **Your restructured output will be passed 
                        to another LLM for the final response generation, so clarity and conciseness are essential.** 

                        ### Instructions:

                        1.  **Relevance Focus:** Identify the key information within each chunk that directly addresses the user's query. 
                        2.  **Conciseness:** Condense the relevant information into a concise summary. Remove any extraneous details or redundant information. 
                        3.  **Synthesis (if applicable):** If multiple chunks contain overlapping or complementary information, synthesize them into a single, more coherent summary. 
                        4.  **Maintain Accuracy:** Ensure the restructured information remains factually accurate and true to the original source documents. 
                            Do not add any information not present in the retrieved chunks. 
                        5.  **Optimized for LLM Consumption:** Structure your output in a way that is easily parsable and digestible by another language model. 
                            This might involve using consistent delimiters, clear headings, or other formatting cues. 
                        6.  **Avoid Conversational Elements:** Focus solely on presenting the restructured information. Do not include any conversational greetings, 
                            summaries, or explanations. The downstream LLM will handle the final presentation to the user. 
                        
                        ### Input:

                        *   **Retrieved Chunks:**
                            ```
                            {documents}
                            ```
                        *   **User Query:**
                            ```
                            {question}
                            ```
        
                        ### Output:
                        
                    """
            prompt = PromptTemplate.from_template(prompt)
            chain = prompt | self.model | StrOutputParser()

            document = chain.invoke({"question": question, "documents": documents})

            return {"retrieved_chunks": document}

    def generate_final_output(self, state: GraphState):
        with st.spinner("Analyzing the retrieved documents and generating response..."):
            documents = state["retrieved_chunks"]
            question = state["user_query"]
            memory = self.memory.load_memory_variables({})

            prompt = """"
                        ### Task: Answer User Queries in a Retrieval Augmented Generation (RAG) Application
                        
                        You are part of a Retrieval Augmented Generation (RAG) application.
                        Your responsibility is to to answer the user queres based on the returned chunks from the vectorstore.
                        You may also use the conversation history as the context. 
                        The returned chunks are derived from the user's input PDF.
                        
                        ### Instructions:
                        
                        1.  Determine if the user query is just greeting. If it is a greeting, respond with a friendly message such as: 
                            "Hello! I'm here to assist you with questions related to the provided PDFs. What would you like to know?"
                        2.  If you deterine the user query is relevant to the returned chunks and conversation history,
                            answer the user query based on these information.
                        
                        ### Remark:
                        
                        Keep your answer concise and to the point.
                        
                        ### Information:
                        
                        * **User Query:**
                        ```
                        {question}
                        ```
                        
                        * **Conversation history:**
                        ```
                        {memory}
                        ```
                        
                        * **Returned chunks:**
                        ```
                        {context}
                        ```
                            
                        ### Output:
                         
                    """
            prompt = PromptTemplate.from_template(prompt)
            chain = prompt | self.model | StrOutputParser()

            response = chain.invoke(
                {"memory": memory, "question": question, "context": documents}
            )

            self.memory.save_context(
                inputs={"input": question}, outputs={"output": response}
            )

            self.graph.get_graph().draw_mermaid_png(output_file_path=r"Flow_graph.png")

            return {"response": response}


# ---------------------------------------------------------------------Deploying the codes to streamlit ----------------------------------------------------------------------

# -----------------------Streamlit page layout Configuration---------------------------------

st.set_page_config(
    page_title="RAG Chatbot",
    page_icon=r"chatbot_icon.jpg",
)

st.image(
    r"AI_pic.jpg",
    width=450,
)

st.subheader("Welcome to Henry's RAG ChatBot")

# -----------------------Display chatbot conversation history---------------------------------

if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "assistant",
            "content": "Please initialize the chatbot on the left sidebar first.",
        }
    ]
    st.session_state.app = None
    st.session_state.not_allow_to_input = True

# -----------------------Side bar configuration---------------------------------

with st.sidebar:

    st.image(
        r"chatbot_icon.jpg",
        width=100,
    )

    st.info(
        """  \n
        Choose the model you would like to use for the RAG.  \n
        The embedding model used is 'nomic-embed-text'. """
    )

    selected_model = st.selectbox(
        "AI Model", ["gemma2", "phi3", "llama3.1", "qwen2.5", "mistral"]
    )

    pdf_files = st.file_uploader(
        "Upload PDFs", type=["pdf"], accept_multiple_files=True
    )


def initialize_ingestor(pdf_files, selected_model):
    retriever = PDF_Importer(pdfs=pdf_files).get_retriever()
    st.success("PDFs successfully uploaded")
    app = PdfChat(retriever, selected_model).graph
    st.success("ChatBot successfully initialized")
    st.session_state.messages.append(
        {
            "role": "assistant",
            "content": "Hello! I'm here to help you with questions about the provided PDFs. What would you like to know?",
        }
    )
    st.session_state.not_allow_to_input = False

    return app


with st.sidebar:
    if st.button("Initialize ChatBot", type="primary"):
        if len(pdf_files) == 0:
            st.error("Please upload PDFs first.")
        else:
            st.session_state.app = initialize_ingestor(pdf_files, selected_model)

app = st.session_state.app


for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# -----------------------Chatbot configuration---------------------------------


def generate_response(question):
    return app.invoke(input={"user_query": question})["response"]


def loading_the_response():
    st.session_state.not_allow_to_input = True
    return


def not_loading_the_response():
    st.session_state.not_allow_to_input = False
    return


if question := st.chat_input(
    placeholder="Ask a question in the pdf",
    disabled=st.session_state.not_allow_to_input,
    on_submit=loading_the_response,
):

    st.chat_message("user").markdown(question)

    st.session_state.messages.append({"role": "user", "content": question})

    with st.chat_message("assistant"):

        response = generate_response(question)

        st.markdown(response)

    not_loading_the_response()

    st.session_state.messages.append({"role": "assistant", "content": response})

    st.rerun()
